{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DebruijnExtend Dataset Analysis\n",
    "This python notebook can be used to perform an analysis of the the datasets being used to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "# in-house\n",
    "import sys\n",
    "sys.path.append(\"/Users/dreyceyalbin/Dropbox/Fall2020-classes/Algorithms/project/DebruijnExtend/py_scripts\")\n",
    "from csvtohash import ProteinHash\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "dataset = \"../data/primary2secondary.csv\"\n",
    "csv_column_names = ['sequence length', 'PDB name', 'Proten Sequence', '8 char', '3 char', '1 char']\n",
    "seq_length_column = 0\n",
    "pdb_name_column = 1\n",
    "protein_column = 2\n",
    "secondary_column = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset, header=None, usecols=[seq_length_column,\n",
    "                                                pdb_name_column, \n",
    "                                                protein_column, \n",
    "                                                secondary_column])\n",
    "df.columns = ['sequence length', 'PDB name', 'Proten Sequence', '3 char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assort by length, drop duplicates proteins/chains (by PDB name)\n",
    "df_sorted = df.sort_values(by = 'sequence length', ascending = False)\n",
    "df_unique = df_sorted.drop_duplicates(subset = [\"PDB name\"])\n",
    "df_unique = df_unique.drop_duplicates()\n",
    "df_unique.duplicated(keep=False).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    393732.000000\n",
       "mean        260.212634\n",
       "std         196.864409\n",
       "min           3.000000\n",
       "25%         131.000000\n",
       "50%         223.000000\n",
       "75%         336.000000\n",
       "max        5037.000000\n",
       "Name: sequence length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Before filtering:\")\n",
    "df[\"sequence length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "393731    '5J8V'\n",
       "393725    '4UWE'\n",
       "393720    '4UWA'\n",
       "393719    '5NUG'\n",
       "393709    '3KGV'\n",
       "           ...  \n",
       "530       '2P7R'\n",
       "454       '1PLW'\n",
       "455       '1PLX'\n",
       "675       '4OLR'\n",
       "689       '4ONK'\n",
       "Name: PDB name, Length: 139496, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After filtering:\")\n",
    "df_unique[\"PDB name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output sequences into a fasta file.\n",
    "def csv_to_fasta(panda_df, fasta_out):\n",
    "    \"\"\"\n",
    "    Turns a pandas DF into a fasta file with\n",
    "    sequences. \n",
    "    \"\"\"\n",
    "    with open(fasta_out, \"w\") as fasta_file:\n",
    "        for index, row in panda_df.iterrows():\n",
    "            sequence = row['Proten Sequence'].strip(\"'\")\n",
    "#             new_seq = \"\"\n",
    "#             for index, character in enumerate(sequence):\n",
    "#                 if character == \"*\":\n",
    "#                     print(\"FUCK\")\n",
    "#                     new_seq += \"X\" # replace * with X\n",
    "#                 else:\n",
    "#                     new_seq += character\n",
    "            name = row['PDB name'].strip(\"'\")\n",
    "            fasta_file.write(f\">{name}\\n{sequence}\\n\")\n",
    "\n",
    "# create output file\n",
    "csv_to_fasta(df_unique, \"unique_proteins.fa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use CD-HIT to create clusters based on identities. \n",
    "\n",
    "Make sure to only run the following cell once as it takes time to run CD-HIT. This clusters the testing/training set based on different identity thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd-hit -i unique_proteins.fa -o unique_95.fa -c 0.95\n",
    "# !cd-hit -i unique_proteins.fa -o unique_90.fa -c 0.9\n",
    "# !cd-hit -i unique_proteins.fa -o unique_85.fa -c 0.85\n",
    "# !cd-hit -i unique_proteins.fa -o unique_80.fa -c 0.80\n",
    "# !cd-hit -i unique_proteins.fa -o unique_75.fa -c 0.75\n",
    "# !cd-hit -i unique_proteins.fa -o unique_70.fa -c 0.70\n",
    "# !cd-hit -i unique_proteins.fa -o unique_65.fa -c 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use clustered file to filter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse clustered sequences to obtain PDB IDs for consensus sequences.\n",
    "# use the parsed PDB IDs to filter the pandas dataframe.\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_fasta_names(fasta):\n",
    "    \"\"\"\n",
    "    retrieves a list of names from a input fasta file\n",
    "    \"\"\"\n",
    "    name_array = {}\n",
    "    with open(fasta) as fasta_file:\n",
    "        fasta_file = fasta_file.readlines()\n",
    "        for line in tqdm(fasta_file):\n",
    "            if line[0] == \">\":\n",
    "                name = line.strip(\"\\n\").strip(\">\").strip(\"'\").strip(\" \")\n",
    "                name = f\"'{name}'\"\n",
    "            name_array[name] = 1\n",
    "        return name_array\n",
    "\n",
    "def filter_df(panda_df, name_dict):\n",
    "    \"\"\"\n",
    "    returns DF with only rows that contain array info\n",
    "    \"\"\"\n",
    "    for index, row in tqdm(panda_df.iterrows()):\n",
    "        name = row['PDB name'].strip(\"'\")\n",
    "        if name not in name_dict.keys():\n",
    "            panda_df.drop(index)\n",
    "    return panda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use clustered proteins to create the testing/training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64354/64354 [00:00<00:00, 1654764.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "393731    '5J8V'\n",
       "393725    '4UWE'\n",
       "393720    '4UWA'\n",
       "393719    '5NUG'\n",
       "393709    '3KGV'\n",
       "           ...  \n",
       "530       '2P7R'\n",
       "454       '1PLW'\n",
       "455       '1PLX'\n",
       "675       '4OLR'\n",
       "689       '4ONK'\n",
       "Name: PDB name, Length: 139496, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dict = list(get_fasta_names('unique_65.fa').keys())\n",
    "#print(name_dict)\n",
    "filtered_df = df_unique[df_unique[\"PDB name\"].isin(name_dict)] #filter_df(df_unique, name_dict)\n",
    "df_unique[\"PDB name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    32177.000000\n",
       "mean       279.213942\n",
       "std        209.337073\n",
       "min         11.000000\n",
       "25%        139.000000\n",
       "50%        239.000000\n",
       "75%        363.000000\n",
       "max       5037.000000\n",
       "Name: sequence length, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[\"sequence length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = df_unique #filtered_df\n",
    "\n",
    "train, test = train_test_split(DATASET, test_size=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>300.808023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>245.063797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>162.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>262.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>369.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4119.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sequence length\n",
       "count       698.000000\n",
       "mean        300.808023\n",
       "std         245.063797\n",
       "min          17.000000\n",
       "25%         162.250000\n",
       "50%         262.500000\n",
       "75%         369.750000\n",
       "max        4119.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save testing and training\n",
    "The below saves the testing and training to the following formats:\n",
    "* Training - pickle\n",
    "* Training - CSV\n",
    "* Testing - Fasta\n",
    "* Testing - CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f'TRAINING.csv', index=False)\n",
    "test.to_csv(f'TESTING.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "609it [00:00, 3059.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138799it [00:56, 2457.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# turn testing into fasta\n",
    "csv_to_fasta(test, \"TESTING.fa\")\n",
    "\n",
    "# turn training into pickle\n",
    "kmer_size = 10\n",
    "prothashOBJ = ProteinHash(f'TRAINING.csv', kmer_size)\n",
    "prothashtable = prothashOBJ.construct_hash()\n",
    "outfile = open(f'TRAINING.pickle','wb')\n",
    "pickle.dump(prothashtable, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " working on fold number: 0\n",
      " Creating training and testing CSVs..\n",
      "Creating a hash tables for the training CSV..\n",
      "WORKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31214it [00:13, 2231.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " working on fold number: 1\n",
      " Creating training and testing CSVs..\n",
      "Creating a hash tables for the training CSV..\n",
      "WORKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31214it [00:17, 1827.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " working on fold number: 2\n",
      " Creating training and testing CSVs..\n",
      "Creating a hash tables for the training CSV..\n",
      "WORKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31215it [00:17, 1817.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " working on fold number: 3\n",
      " Creating training and testing CSVs..\n",
      "Creating a hash tables for the training CSV..\n",
      "WORKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31215it [00:17, 1784.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " working on fold number: 4\n",
      " Creating training and testing CSVs..\n",
      "Creating a hash tables for the training CSV..\n",
      "WORKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31215it [00:17, 1786.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into testing and training\n",
    "### 1.A. create CSV outputs for all - testing_[N].csv, training_[N].csv\n",
    "### 1.B. create hash table for training - training_[N].p\n",
    "# Parameters\n",
    "NUMBER_OF_FOLDS = 5\n",
    "kmer_size = 3\n",
    "\n",
    "kf = KFold(n_splits = NUMBER_OF_FOLDS) #, shuffle = True, random_state = 2)\n",
    "for K, fold in enumerate(kf.split(filtered_df)):\n",
    "    print(f\"\\n working on fold number: {K}\")\n",
    "    train, test = 0, 0\n",
    "    train = df_unique.iloc[fold[0]]\n",
    "    test =  df_unique.iloc[fold[1]]\n",
    "    print(f\" Creating training and testing CSVs..\")\n",
    "    train.to_csv(f'training_{K+1}.csv', index=False)\n",
    "    test.to_csv(f'testing_{K+1}.csv', index=False)\n",
    "    csv_to_fasta(test, f'testing_{K+1}.fa')\n",
    "    # print(train[\"PDB name\"].duplicated().sum())\n",
    "    # print(test[\"PDB name\"].duplicated().sum())\n",
    "    # print(train[\"PDB name\"].count() + test[\"PDB name\"].count())\n",
    "    # print(test[\"PDB name\"].count())\n",
    "    # print(df_unique[\"PDB name\"].count())\n",
    "    print(f\"Creating a hash tables for the training CSV..\")\n",
    "    prothashOBJ = ProteinHash(f'training_{K+1}.csv', kmer_size)\n",
    "    prothashtable = prothashOBJ.construct_hash()\n",
    "    outfile = open(f'testing_{K+1}.pickle','wb')\n",
    "    pickle.dump(prothashtable, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "## 1. turn the splits into K sets of testing/training\n",
    "### 1.A. create CSV outputs for all - testing_[N].csv, training_[N].csv\n",
    "### 1.B. create hash table for training - training_[N].p\n",
    "### 1.C. create fasta file input for the testing - testing_[N].fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence length</th>\n",
       "      <th>PDB name</th>\n",
       "      <th>Proten Sequence</th>\n",
       "      <th>3 char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>'1A30'</td>\n",
       "      <td>'EDL'</td>\n",
       "      <td>'CEC'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>'1B05'</td>\n",
       "      <td>'KCK'</td>\n",
       "      <td>'CEC'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>'1B0H'</td>\n",
       "      <td>'KAK'</td>\n",
       "      <td>'CEC'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>'1B2H'</td>\n",
       "      <td>'KAK'</td>\n",
       "      <td>'CEC'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>'1B32'</td>\n",
       "      <td>'KMK'</td>\n",
       "      <td>'CEC'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139491</th>\n",
       "      <td>166</td>\n",
       "      <td>'1G2I'</td>\n",
       "      <td>'MKVLFLTANEFEDVELIYPYHRLKEEGHEVYIASFERGTITGKHG...</td>\n",
       "      <td>'CEEEEECCCCECHHHHHHHHHHHHHHCCEEEEEECCCEEEECCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139492</th>\n",
       "      <td>166</td>\n",
       "      <td>'1G5M'</td>\n",
       "      <td>'MAHAGRTGYDNREIVMKYIHYKLSQRGYEWDAGDDVEENRTEAPE...</td>\n",
       "      <td>'CCCCCCCCCCHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139493</th>\n",
       "      <td>166</td>\n",
       "      <td>'1GJH'</td>\n",
       "      <td>'MAHAGRTGYDNREIVMKYIHYKLSQRGYEWDAGDDVEENRTEAPE...</td>\n",
       "      <td>'CCCCCCCCCCHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139494</th>\n",
       "      <td>166</td>\n",
       "      <td>'1GNP'</td>\n",
       "      <td>'MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVV...</td>\n",
       "      <td>'CEEEEEEEECCCCCCHHHHHHHHHHCCCCCCCCCCCEEEEEEEEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139495</th>\n",
       "      <td>166</td>\n",
       "      <td>'1GNQ'</td>\n",
       "      <td>'MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVV...</td>\n",
       "      <td>'CEEEEEEEEECCCCCHHHHHHHHCCCCCCCCCCCCCCEEEEEEEE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125547 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sequence length PDB name  \\\n",
       "0                     3   '1A30'   \n",
       "1                     3   '1B05'   \n",
       "2                     3   '1B0H'   \n",
       "4                     3   '1B2H'   \n",
       "5                     3   '1B32'   \n",
       "...                 ...      ...   \n",
       "139491              166   '1G2I'   \n",
       "139492              166   '1G5M'   \n",
       "139493              166   '1GJH'   \n",
       "139494              166   '1GNP'   \n",
       "139495              166   '1GNQ'   \n",
       "\n",
       "                                          Proten Sequence  \\\n",
       "0                                                   'EDL'   \n",
       "1                                                   'KCK'   \n",
       "2                                                   'KAK'   \n",
       "4                                                   'KAK'   \n",
       "5                                                   'KMK'   \n",
       "...                                                   ...   \n",
       "139491  'MKVLFLTANEFEDVELIYPYHRLKEEGHEVYIASFERGTITGKHG...   \n",
       "139492  'MAHAGRTGYDNREIVMKYIHYKLSQRGYEWDAGDDVEENRTEAPE...   \n",
       "139493  'MAHAGRTGYDNREIVMKYIHYKLSQRGYEWDAGDDVEENRTEAPE...   \n",
       "139494  'MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVV...   \n",
       "139495  'MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVV...   \n",
       "\n",
       "                                                   3 char  \n",
       "0                                                   'CEC'  \n",
       "1                                                   'CEC'  \n",
       "2                                                   'CEC'  \n",
       "4                                                   'CEC'  \n",
       "5                                                   'CEC'  \n",
       "...                                                   ...  \n",
       "139491  'CEEEEECCCCECHHHHHHHHHHHHHHCCEEEEEECCCEEEECCCC...  \n",
       "139492  'CCCCCCCCCCHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCC...  \n",
       "139493  'CCCCCCCCCCHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCC...  \n",
       "139494  'CEEEEEEEECCCCCCHHHHHHHHHHCCCCCCCCCCCEEEEEEEEE...  \n",
       "139495  'CEEEEEEEEECCCCCHHHHHHHHCCCCCCCCCCCCCCEEEEEEEE...  \n",
       "\n",
       "[125547 rows x 4 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (Benchmarking):\n",
    "## 1. create a testing/training split.\n",
    "### 1.A. Create a training set (N=?)\n",
    "### 1.B. Create a testing set that does not overlap training (N=100)\n",
    "## 2. Benchmarking the tools.\n",
    "### 2.A. Download several tools, ensure each tool can be downloaded/installed with a button push.\n",
    "### 2.B. Automate the benchmarking with a BASH script or python script with subcalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=4 : CCCCECCCCCCCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHHHHHHHCEEEEECCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHCHHHHHHHHHHHHHHHHHHHCCCHHHHHHHHHCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHHHCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCHHHHHHHHHHHHHHHCC\n",
    "k=7 : CCCCECCCCCCCCCCCEECCEHHHHCCCHHHHHHHHHHHHHHHHCCCCCCCCHHHHHHHHCCCCCCCCECCCCCCCCCCCCCCCCCCCHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCHHHHHHHHHCCCCCECCCCCCCCCCCCCCCCHHHHCCCEEEEECCCCCEEEEECCCCCCECCCCCCCCCHHHHCCCECCCCECCCCCECHHHHHHHHHHHHHHHHHHHHHHHHHCCCHHHHHCCCCCCCCCCCCHHHHHHHHHHHHHCCCCCCCCCCCHHHCCCCHHHCCHHHHHHHHHHHHHHHHHHHCCCCCCCCEEECCCCCCCCCCEEECCCCEEEEEECCCCCEEEHHHHCCHHHHCEECHHHCCCCCCCHHHHHHHHHHHHHHHHC\n",
    "k=21: CCCCECCCCCCCCCCCEECCEHHHHCCCHHHHHHHHHHHHHHHHCCCCCCCCHHHHHHHHCCCCCCCCECCCCCCCCCCCCCCCCCCCHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCHHHHHHHHHCCCCCECCCCCCCCCCCCCCCCHHHHCCCEEEEECCCCCEEEEECCCCCCECCCCCCCCCHHHHCCCECCCCECCCCCECHHHHHHHHHHHHHHHHHHHHHHHHHCCCHHHHHCCCCCCCCCCCCHHHHHHHHHHHHHCCCCCCCCCCCHHHCCCCHHHCCHHHHHHHHHHHHHHHHHHHCCCCCCCCEEECCCCCCCCCCEEECCCCEEEEEECCCCCEEEHHHHCCHHHHCEECHHHCCCCCCCHHHHHHHHHHHHHHHHC\n",
    "k=31: CCCCECCCCCCCCCCCEECCEHHHHCCCHHHHHHHHHHHHHHHHCCCCCCCCHHHHHHHHCCCCCCCCECCCCCCCCCCCCCCCCCCCHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCHHHHHHHHHCCCCCECCCCCCCCCCCCCCCCHHHHCCCEEEEECCCCCEEEEECCCCCCECCCCCCCCCHHHHCCCECCCCECCCCCECHHHHHHHHHHHHHHHHHHHHHHHHHCCCHHHHHCCCCCCCCCCCCHHHHHHHHHHHHHCCCCCCCCCCCHHHCCCCHHHCCHHHHHHHHHHHHHHHHHHHCCCCCCCCEEECCCCCCCCCCEEECCCCEEEEEECCCCCEEEHHHHCCHHHHCEECHHHCCCCCCCHHHHHHHHHHHHHHHHC"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37f8066621b9c20950eb2eab583199745a68ce775e2d71e73e7181daa55112e7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
